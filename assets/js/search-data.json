{
  
    
        "post0": {
            "title": "Rank of a Matrix",
            "content": "Let&#39;s think of the rank of a matrix as a measure of its &quot;dimensionality&quot; or &quot;independent directions.&quot; . Imagine you have a set of vectors. If these vectors point in completely different directions (are independent of each other), then they can span a certain space. The rank of a matrix essentially tells you how many independent directions or dimensions this matrix can define. . Here&#39;s a simple way to look at it: . Vectors in Space: Think of each row of a matrix as a vector. If one row is a linear combination (a combination like (2 times text{row}_1 + 3 times text{row}_2)) of other rows, then it&#39;s not adding a new &quot;direction&quot; or &quot;information&quot; to our set. . | Rank and Dimensions: The rank of a matrix is like counting how many truly independent directions (or dimensions) these rows (vectors) span. If you have a matrix with a rank of 3, it means that the information in that matrix spans a three-dimensional space. No more, no less. . | Applications: . Linear Systems: In systems of equations, the rank can help you determine if you have a unique solution, no solution, or infinitely many solutions. | Data Analysis: In data science, understanding the rank of a data matrix can help determine its dimensionality and how much information is captured by the data. | Optimization: In various optimization problems, the rank can provide insights into the structure of the problem. | . | In essence, the rank of a matrix provides a compact way to understand the &quot;effective&quot; dimensionality or independence of the information it represents. . import torch import matplotlib.pyplot as plt # Define a 2x2 matrix with determinant != 0 (full rank) A = torch.tensor([[2, 3], [1, 2]], dtype=torch.float32) # Define a 2x2 matrix with one row being a multiple of the other (rank 1) B = torch.tensor([[1, 2], [2, 4]], dtype=torch.float32) # Generate 100 points in a unit circle theta = torch.linspace(0, 2 * 3.141592, 100) x = torch.cos(theta) y = torch.sin(theta) points = torch.stack([x, y]) # Transform points using matrix A transformed_A = torch.matmul(A, points) x_A, y_A = transformed_A[0], transformed_A[1] # Transform points using matrix B transformed_B = torch.matmul(B, points) x_B, y_B = transformed_B[0], transformed_B[1] # Plotting plt.figure(figsize=(12, 5)) # Plot for matrix A (full rank) plt.subplot(1, 2, 1) plt.title(&#39;Matrix A: Rank 2 (Full Rank)&#39;) plt.scatter(x, y, label=&#39;Original Points&#39;) plt.scatter(x_A, y_A, label=&#39;Transformed Points&#39;) plt.legend() plt.axis(&#39;equal&#39;) # Plot for matrix B (rank 1) plt.subplot(1, 2, 2) plt.title(&#39;Matrix B: Rank 1&#39;) plt.scatter(x, y, label=&#39;Original Points&#39;) plt.scatter(x_B, y_B, label=&#39;Transformed Points&#39;) plt.legend() plt.axis(&#39;equal&#39;) plt.tight_layout() . import torch import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D # Rank 1 matrix (very simplified representation) B_rank1 = torch.tensor([[1, 0, 0], [0, 0, 0], [0, 0, 0]], dtype=torch.float32) # Rank 2 matrix (very simplified representation) C_rank2 = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 0]], dtype=torch.float32) # Generate 3D points on a unit sphere phi = torch.linspace(0, 2 * 3.141592, 100) theta = torch.linspace(0, 3.141592, 50) phi, theta = torch.meshgrid(phi, theta) x = torch.sin(theta) * torch.cos(phi) y = torch.sin(theta) * torch.sin(phi) z = torch.cos(theta) points = torch.stack([x.flatten(), y.flatten(), z.flatten()]) # Transform points using matrices transformed_rank1 = torch.matmul(B_rank1, points) transformed_rank2 = torch.matmul(C_rank2, points) # Plotting fig = plt.figure(figsize=(20, 6)) # Original Points (no transformation) ax1 = fig.add_subplot(1, 4, 1, projection=&#39;3d&#39;) ax1.scatter(points[0], points[1], points[2], label=&#39;Original Points&#39;, alpha=0.5, c=&#39;r&#39;) ax1.set_title(&#39;Original Points&#39;) ax1.set_xlabel(&#39;X&#39;) ax1.set_ylabel(&#39;Y&#39;) ax1.set_zlabel(&#39;Z&#39;) ax1.legend() # Rank 1 Transformation ax2 = fig.add_subplot(1, 4, 2, projection=&#39;3d&#39;) ax2.scatter(transformed_rank1[0], transformed_rank1[1], transformed_rank1[2], label=&#39;Transformed Points&#39;, alpha=0.5, c=&#39;b&#39;) ax2.set_title(&#39;Rank 1 Transformation&#39;) ax2.set_xlabel(&#39;X&#39;) ax2.set_ylabel(&#39;Y&#39;) ax2.set_zlabel(&#39;Z&#39;) ax2.legend() # Rank 2 Transformation ax3 = fig.add_subplot(1, 4, 3, projection=&#39;3d&#39;) ax3.scatter(transformed_rank2[0], transformed_rank2[1], transformed_rank2[2], label=&#39;Transformed Points&#39;, alpha=0.5, c=&#39;b&#39;) ax3.set_title(&#39;Rank 2 Transformation&#39;) ax3.set_xlabel(&#39;X&#39;) ax3.set_ylabel(&#39;Y&#39;) ax3.set_zlabel(&#39;Z&#39;) ax3.legend() # Rank 3 Transformation (from the previous example) ax4 = fig.add_subplot(1, 4, 4, projection=&#39;3d&#39;) ax4.scatter(transformed_A[0], transformed_A[1], transformed_A[2], label=&#39;Transformed Points&#39;, alpha=0.5, c=&#39;b&#39;) ax4.set_title(&#39;Rank 3 Transformation&#39;) ax4.set_xlabel(&#39;X&#39;) ax4.set_ylabel(&#39;Y&#39;) ax4.set_zlabel(&#39;Z&#39;) ax4.legend() plt.tight_layout() .",
            "url": "https://deepsworld.github.io/blog/linear_algebra/2023/12/26/Rank-of-a-Matrix.html",
            "relUrl": "/linear_algebra/2023/12/26/Rank-of-a-Matrix.html",
            "date": " • Dec 26, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Linear Transformation",
            "content": "import numpy as np import matplotlib.pyplot as plt . x = np.arange(-10, 10, 1) y = np.arange(-10, 10, 1) xx, yy = np.meshgrid(x, y) . plt.scatter(xx, yy, s=30, c=xx+yy) # [...] Add axis, x and y witht the same scale . &lt;matplotlib.collections.PathCollection at 0x1227627f0&gt; . T = np.array([ [-1, 0], [0, -1] ]) . xy = np.vstack([xx.flatten(), yy.flatten()]) xy.shape . (2, 400) . trans = T @ xy trans.shape . (2, 400) . xx_transformed = trans[0].reshape(xx.shape) yy_transformed = trans[1].reshape(yy.shape) . f, axes = plt.subplots(1, 2, figsize=(6, 3)) axes[0].scatter(xx, yy, s=10, c=xx+yy) axes[1].scatter(xx_transformed, yy_transformed, s=10, c=xx+yy) # [...] Add axis, x and y witht the same scale . &lt;matplotlib.collections.PathCollection at 0x1228cc4f0&gt; . T = np.array([ [1.3, -2.4], [0.1, 2] ]) trans = T @ xy xx_transformed = trans[0].reshape(xx.shape) yy_transformed = trans[1].reshape(yy.shape) f, axes = plt.subplots(1, 2, figsize=(6, 3)) axes[0].scatter(xx, yy, s=10, c=xx+yy) axes[1].scatter(xx_transformed, yy_transformed, s=10, c=xx+yy) # [...] Add axis, x and y witht the same scale . &lt;matplotlib.collections.PathCollection at 0x12297f6d0&gt; . # T_inv @ T @ v = I @ v = v T = np.array([ [1.3, -2.4], [0.1, 2] ]) trans = T @ xy T_inv = np.linalg.inv(T) un_trans = T_inv @ T @ xy f, axes = plt.subplots(1, 3, figsize=(9, 3)) axes[0].scatter(xx, yy, s=10, c=xx+yy) axes[1].scatter(trans[0].reshape(xx.shape), trans[1].reshape(yy.shape), s=10, c=xx+yy) axes[2].scatter(un_trans[0].reshape(xx.shape), un_trans[1].reshape(yy.shape), s=10, c=xx+yy) . &lt;matplotlib.collections.PathCollection at 0x122a6f2e0&gt; . # transformation by a singular matrix cannot be reset because the points # land on each other in the same space hence non retrievable T = np.array([ [3, 6], [2, 4], ]) trans = T @ xy f, axes = plt.subplots(1, 2, figsize=(6, 3)) axes[0].scatter(xx, yy, s=10, c=xx+yy) axes[1].scatter(trans[0].reshape(xx.shape), trans[1].reshape(yy.shape), s=10, c=xx+yy) # [...] Add axis, x and y witht the same scale . &lt;matplotlib.collections.PathCollection at 0x122b0d9a0&gt; .",
            "url": "https://deepsworld.github.io/blog/linear_algebra/2022/09/14/Linear-Transformation.html",
            "relUrl": "/linear_algebra/2022/09/14/Linear-Transformation.html",
            "date": " • Sep 14, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Probabilities Notes",
            "content": "Random Variables . Random variables are the variables that can take multiple values depending on the result of a random event. e.g. Throwing a dice, tossing a coin, height of a person, size of a rock, etc. . These can be Discrete: fixed number of values it can take, for example, tossing a coin can take only 2 values, heads or tails . or Continuous: any(infine) number of possible values, for example, size of a rock, could be as small as a bug or as large as the mount everest. . Probabilty Distribution . The probability of a random variable describes the probability of each possible result or outcome. In case of a coin toss the probability of both heads and tails is 1/2 so, the probability distribution is 1/2, 1/2. The PD always sums to 1. Higher probaility implies more likely . The probaility distribution of discrete random variable is called Probaility Mass Function | The PD of continuous random variable is called Probabilty Density Function | . import numpy as np import random import matplotlib.pyplot as plt # Probaility Mass Function of &quot;Throwing a Dice&quot; num_throws = 10000 outcomes = np.zeros(num_throws) discrete_outcomes = [1, 2, 3, 4, 5, 6] for i in range(num_throws): outcomes[i] = random.choice(discrete_outcomes) val, counts = np.unique(outcomes, return_counts=True) prob = counts / num_throws plt.bar(val, prob) plt.ylabel(&quot;Probability&quot;) plt.xlabel(&quot;Outcome&quot;) plt.show() plt.close() # verify the output is close enough to theoretical value np.allclose(prob, np.ones(len(prob)) * 1/ 6, atol=0.05) . True . An example of probabilty distributions of 2 discrete random variables . Joint Probability . P(X=x, Y=y) What is the probabilty of throwing a dice and getting 1 and tossing a coin and getting head or tail . Conditional Probability . P(X=x | Y=y) What is the probability of throwing a dice and getting 1 given that tossing a coing resulted in a head . Marginal Probability . P(X=x) What is the probabilty of throwing a dice a getting 1 probabilty distribution of subset of variables . Uniform Distribution . Each value us qually likely to occur. In other words, the probabilty of each value is equal . import seaborn as sns # An example of uniform continuous distribution x = np.random.uniform(0, 0.5, 10000) sns.distplot(x) plt.show() . /usr/local/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . Continuous Variables . np.random.seed(123) # draw 1000 values from a normal distribution with mean=0 and std=1 # drawing values implies that the output values will always satisfy the constrains of the normal distribution # these constrains in this case are when we find their mean, it will be 0 and their standard deviation will # be 1 x = np.random.normal(0, 1, 1000) x.shape . (1000,) . x.mean() . -0.03956413608079184 . x.std() # nice ! . 1.0007875375162334 . np.random.seed(123) x = np.random.normal(0, 1, 1000) y = np.random.normal(0, 1, 1000) sns.distplot(x) plt.title(&#39;x&#39;) plt.xlim(-4, 4) plt.show() sns.distplot(y) plt.title(&#39;y&#39;) plt.xlim(-4, 4) plt.show() . /usr/local/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . /usr/local/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . NOTE: the y-axis here represents the probaility density and the x-axis is the continuous values . Conditional Events can be of 2 types . Dependent Events: Drawing 2 cards from the same deck without replacement. . | Independent Events: One event does not affect the other such as, Throwing a dice and tossing a coin . | .",
            "url": "https://deepsworld.github.io/blog/probability/2022/08/06/Probability-Notes.html",
            "relUrl": "/probability/2022/08/06/Probability-Notes.html",
            "date": " • Aug 6, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Norms",
            "content": ".",
            "url": "https://deepsworld.github.io/blog/linear_algebra/2022/08/06/Norms.html",
            "relUrl": "/linear_algebra/2022/08/06/Norms.html",
            "date": " • Aug 6, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Matrices Notes",
            "content": "# solve systems of linear equations import numpy as np &#39;&#39;&#39; 3x - y = 7 2x + y = 8 - 5x = 15 x = 3; y = 2 &#39;&#39;&#39; mat = np.array([[3, -1], [2, 1]]) # 2 x 2 rhs = np.array([[7, 8]]) # 1 x 2 out = np.linalg.inv(mat) @ rhs.transpose() # (2 x 2) @ (1 x 2).T out . array([[3.], [2.]]) . I = np.eye(3, 3) I . array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . &#39;&#39;&#39; Ainv @ A = I &#39;&#39;&#39; mat = np.random.rand(2, 2) I = np.linalg.inv(mat) @ mat I . array([[1.00000000e+00, 3.32227679e-16], [3.47665962e-18, 1.00000000e+00]]) . NOTE: The output matrix does not look exactly what we expect, this is because of the limitation of floating point precision and for practical purposes e-16 can be considered as zero or very close to zero . singular = np.array([[1, 0], [0, 0]]) singular . array([[1, 0], [0, 0]]) . try: np.linalg.inv(singular) except Exception as e: print(f&#39;Cannot find inverse because: {e}&#39;) . Cannot find inverse because: Singular matrix . det_singular = np.linalg.det(singular) det_singular . 0.0 .",
            "url": "https://deepsworld.github.io/blog/linear_algebra/2022/08/06/Matrices-Notes.html",
            "relUrl": "/linear_algebra/2022/08/06/Matrices-Notes.html",
            "date": " • Aug 6, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Linear Dependence & Span",
            "content": "import matplotlib.pyplot as plt import numpy as np . def plotVectors(vecs, cols, alpha=1): &quot;&quot;&quot; Plot set of vectors. Parameters - vecs : array-like Coordinates of the vectors to plot. Each vectors is in an array. For instance: [[1, 3], [2, 2]] can be used to plot 2 vectors. cols : array-like Colors of the vectors. For instance: [&#39;red&#39;, &#39;blue&#39;] will display the first vector in red and the second in blue. alpha : float Opacity of vectors Returns: fig : instance of matplotlib.figure.Figure The figure of the vectors &quot;&quot;&quot; plt.figure() plt.axvline(x=0, color=&#39;#A9A9A9&#39;, zorder=0) plt.axhline(y=0, color=&#39;#A9A9A9&#39;, zorder=0) for i in range(len(vecs)): x = np.concatenate([[0,0],vecs[i]]) plt.quiver([x[0]], [x[1]], [x[2]], [x[3]], angles=&#39;xy&#39;, scale_units=&#39;xy&#39;, scale=1, color=cols[i], alpha=alpha) . orange = &#39;#FF9A13&#39; blue = &#39;#1190FF&#39; u = [1, 3] v = [2, 1] plotVectors([u, v], [orange, blue]) plt.xlim(0, 5) plt.ylim(0, 5) . (0.0, 5.0) . a = 2 b = 1 # Start and end coordinates of the vectors u = [0,0,1,3] v = [2,6,2,1] plt.quiver([u[0], a*u[0], b*v[0]], [u[1], a*u[1], b*v[1]], [u[2], a*u[2], b*v[2]], [u[3], a*u[3], b*v[3]], angles=&#39;xy&#39;, scale_units=&#39;xy&#39;, scale=1, color=[orange, orange, blue]) plt.xlim(-1, 8) plt.ylim(-1, 8) # Draw axes plt.axvline(x=0, color=&#39;#A9A9A9&#39;) plt.axhline(y=0, color=&#39;#A9A9A9&#39;) plt.scatter(4,7,marker=&#39;x&#39;,s=50) # Draw the name of the vectors plt.text(-0.5, 2, r&#39;$ vec{u}$&#39;, color=orange, size=18) plt.text(0.5, 4.5, r&#39;$ vec{u}$&#39;, color=orange, size=18) plt.text(2.5, 7, r&#39;$ vec{v}$&#39;, color=blue, size=18) plt.show() plt.close() . Adding 2 vectors: linear combination or weighted sum . a * np.array(u) + b * np.array(v) . array([2, 6, 4, 7]) . Linear dependence . From: https://math.stackexchange.com/a/2780506/1086132 . Here&#39;s my intuition: Think of vectors as the axes that we use to define a two-dimensional, three-dimensional, or $n$-dimensional space. A set of vectors is linearly dependent when one of the vectors isn&#39;t necessary — it doesn&#39;t add anything useful to our coordinate system. This happens when one vector is just a linear combination of other vectors in the set. . As an example, consider these three vectors in two-dimensional space: . $ vec w = begin{bmatrix}2 3 end{bmatrix}$, $ vec x = begin{bmatrix}1 0 end{bmatrix}$, $ vec y = begin{bmatrix}0 1 end{bmatrix}$ . In this case, we can express $ vec w$ as a linear combination of the other two vectors, $ vec w = 2 vec x + 3 vec y$: . $ begin{bmatrix}2 3 end{bmatrix} = 2 cdot begin{bmatrix}1 0 end{bmatrix} + 3 cdot begin{bmatrix}0 1 end{bmatrix}$ . Equivalently, we can rewrite this as $2 vec x + 3 vec y - 1 vec w = 0$: . $2 cdot begin{bmatrix}1 0 end{bmatrix} + 3 cdot begin{bmatrix}0 1 end{bmatrix} - 1 begin{bmatrix}2 3 end{bmatrix} = begin{bmatrix}0 0 end{bmatrix}$ . Intuitively, this is what is meant by the proof. When these factors hold, it means that at least one vector is essentially &quot;redundant&quot; and unnecessary — it&#39;s just a combination of the other vectors. . More generally, assume you have a set of vectors with this property: . $a_1 vec v_1 + a_2 vec v_2 + ... + a_n vec v_n = 0 $ . This can be written as: . $-a_1 vec v_1 = a_2 vec v_2 + ... + a_n vec v_n$ . Which means that $v_1$ can be expressed as simply a linear combination of the other vectors: . $v_1 = a_2&#39; v_2 + ... + a_n&#39;v_n$ . When you have a set of vectors like this, it means that you have multiple ways to express the same point using your coordinate system. For example, with the vectors above, a point could be identified in the standard x/y coordinate system as (8, 15). But the same point could also be expressed as (4, 3) in w/y coordinates or as (5, -2) in w/x coordinates: . $8 vec x + 15 vec y = 8 begin{bmatrix}1 0 end{bmatrix} + 15 begin{bmatrix}0 1 end{bmatrix} = begin{bmatrix}8 15 end{bmatrix}$ . $4 vec w + 3 vec y = 4 begin{bmatrix}2 3 end{bmatrix} + 3 begin{bmatrix}0 1 end{bmatrix} = begin{bmatrix}8 15 end{bmatrix}$ . $5 vec w -2 vec x = 5 begin{bmatrix}2 3 end{bmatrix} -2 begin{bmatrix}1 0 end{bmatrix} = begin{bmatrix}8 15 end{bmatrix}$ . We&#39;d normally rather have a minimal set of vectors so that there&#39;s only one way to represent each point. It keeps the math simpler. This is similar to a system of equations in which you have three equations for two unknowns. In the above, since we have $ vec w = 2 vec x + 3 vec y$, we can always substitute $ vec x$ and $ vec y$ for $ vec w$. . Note that there&#39;s no &quot;correct&quot; coordinate system -- any set of two vectors here, $ { vec x, vec y }$, $ { vec w, vec y }$, or $ { vec w, vec x }$, would provide a valid basis for a two-dimensional coordinate system. But there&#39;s no need to have three vectors here to represent a point in two-dimensional space. . Span . All the points we can reach by combininig u and v and changing a and b. This set of points is the span of the vectors u and v .",
            "url": "https://deepsworld.github.io/blog/linear_algebra/2022/08/06/Linear-Dependence-&-Span.html",
            "relUrl": "/linear_algebra/2022/08/06/Linear-Dependence-&-Span.html",
            "date": " • Aug 6, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Eigen Decomposition",
            "content": "import matplotlib.pyplot as plt import numpy as np . def plotVectors(vecs, cols, alpha=1): &quot;&quot;&quot; Plot set of vectors. Parameters - vecs : array-like Coordinates of the vectors to plot. Each vectors is in an array. For instance: [[1, 3], [2, 2]] can be used to plot 2 vectors. cols : array-like Colors of the vectors. For instance: [&#39;red&#39;, &#39;blue&#39;] will display the first vector in red and the second in blue. alpha : float Opacity of vectors Returns: fig : instance of matplotlib.figure.Figure The figure of the vectors &quot;&quot;&quot; plt.figure() plt.axvline(x=0, color=&#39;#A9A9A9&#39;, zorder=0) plt.axhline(y=0, color=&#39;#A9A9A9&#39;, zorder=0) for i in range(len(vecs)): x = np.concatenate([[0,0],vecs[i]]) plt.quiver([x[0]], [x[1]], [x[2]], [x[3]], angles=&#39;xy&#39;, scale_units=&#39;xy&#39;, scale=1, color=cols[i], alpha=alpha) . u = np.array([1.5, 1]) A = np.array([ [1.2, 0.9], [0, -0.4] ]) v = A @ u plotVectors([u, v], cols=[&#39;red&#39;, &#39;blue&#39;]) plt.xlim(-3, 3) plt.ylim(-2, 2) . (-2.0, 2.0) . x = np.array([-0.4902, 0.8715]) y = A @ x plotVectors([x, y], cols=[&#39;red&#39;, &#39;blue&#39;]) plt.xlim(-1, 1) plt.ylim(-0.4, 0.9) . (-0.4, 0.9) . The vector x has a special relationship with the matrix A: it is rescaled (with a negative value), but both the initial vector x and the transformed vector y are on the same line. . The vector x is an eigenvector of A. It is only scaled by a value, which is called an eigenvalue of the matrix A. An eigenvector of the matrix A is a vector that is contracted or elongated when transformed by the matrix. The eigenvalue is the scaling factor by which the vector is contracted or elongated. . mA vx = lambda vx .",
            "url": "https://deepsworld.github.io/blog/linear_algebra/2022/08/06/Eigen-Decomposition.html",
            "relUrl": "/linear_algebra/2022/08/06/Eigen-Decomposition.html",
            "date": " • Aug 6, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Basis and Change of Basis",
            "content": "import matplotlib.pyplot as plt import numpy as np . Basis . Basis is a coordinate system used to describe a vector space or a set of vector. Any vector in this coordinate system can be created using the weighted linear combination of the basis vectors. . To be considered as basis vector, a set of vectors: . must span the entire space | must be linearly independent | . NOTE: Span of the set of vectors is all the points we can reach by the weighted linear combination of the vectors in the set. If the set of vectors are not linearly independent then there is a redundant vector which can be removed and we can still reach all the points in the span. . So, If we have a set of vectors with which we can reach all the points in the span of the basis vector without having any redundant vector in the set, it can be called Basis. . Every vector in the space if the unique combination of the basis vectors. | The size of the basis vector set is the dimension of the space. | There are two basis vectors in R2 (corresponding to the x and y-axis in the Cartesian plane), or three in R3. | If the number of vectors in a set is larger than the dimensions of the space, they can’t be linearly independent. | If a set contains fewer vectors than the number of dimensions, these vectors can’t span the whole space. | In a cartesian plane, basis vectors are the the orthogonal unit vectors: genrally denoted as i and j. | . NOTE: Basis vectors can be orthogonal because orthogonal vectors are independent. However, the converse is not necessarily true: non-orthogonal vectors can be linearly independent and thus form a basis (but not a standard basis). . Change of Basis . Change of basis can be used to go from one basis to another i.e from one set of basis vectors to another set. . NOTE: Change of basis vs linear transformation The difference between change of basis and linear transformation is conceptual. Sometimes it is useful to consider the effect of a matrix as a change of basis; sometimes you get more insights when you think of it as a linear transformation. Either you move the vector or you move its reference. This is why rotating the coordinate system has an inverse effect compared to rotating the vector itself. For eigendecomposition and SVD, both of these views are usually taken together, which can be confusing at first. Keeping this difference in mind will be useful throughout the end of the book. The main technical difference between the two is that change of basis must be invertible, which is not required for linear transformations. . # This means that if you go to 0.86757991i′−1.00456621j′ you arrive to the position (2, 1) v_B1 = np.array([2, 1]) B_2 = np.array([ [0.8, -1.3], [1.5, 0.3] ]) v_B2 = np.linalg.inv(B_2) @ v_B1 v_B2 . array([ 0.86757991, -1.00456621]) . B_2 @ v_B2 . array([2., 1.]) .",
            "url": "https://deepsworld.github.io/blog/linear_algebra/2022/08/06/Basis-and-Change-of-Basis.html",
            "relUrl": "/linear_algebra/2022/08/06/Basis-and-Change-of-Basis.html",
            "date": " • Aug 6, 2022"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://deepsworld.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}